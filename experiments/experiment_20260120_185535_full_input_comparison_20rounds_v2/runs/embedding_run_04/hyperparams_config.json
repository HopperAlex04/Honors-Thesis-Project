{
  "comment": "Hyperparameter configuration for training DQN agent in Cuttle game.",
  "random_seed": 16024,
  "comment_random_seed": "Seed for reproducibility. Set to null for non-deterministic runs. Experiment manager overrides this.",
  "network_type": "embedding",
  "comment_network_type": "Options: 'boolean', 'embedding', 'multi_encoder'",
  "embedding_dim": 32,
  "comment_embedding_dim": "For embedding-based network only",
  "embedding_size": 16,
  "comment_embedding_size": "Kept for backward compatibility, no longer used (no embeddings - all inputs are boolean arrays)",
  "batch_size": 128,
  "gamma": 0.9,
  "eps_start": 0.9,
  "eps_end": 0.05,
  "eps_decay": 1500,
  "eps_decay_comment": "Decay constant for epsilon-greedy. With 5000 training episodes, eps reaches ~0.08 by end of training.",
  "tau": 0.005,
  "target_update_frequency": 500,
  "learning_rate": 0.0001,
  "lr_decay_rate": 0.9,
  "lr_decay_interval": 5,
  "gradient_clip_norm": 5.0,
  "q_value_clip": 15.0,
  "replay_buffer_size": 30000,
  "training": {
    "rounds": 20,
    "eps_per_round": 250,
    "quick_test_mode": false,
    "quick_test_rounds": 1,
    "quick_test_eps_per_round": 100,
    "validation_episodes_ratio": 1.0,
    "validation_opponent": "both",
    "validation_opponent_comment": "Options: 'randomized', 'gapmaximizer', or 'both'"
  },
  "early_stopping": {
    "enabled": true,
    "check_interval": 50,
    "window_size": 100,
    "divergence_threshold": 0.5,
    "min_episodes": 200,
    "max_loss": 50.0
  }
}