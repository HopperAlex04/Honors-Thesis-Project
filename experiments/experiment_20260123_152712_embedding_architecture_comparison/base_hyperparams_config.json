{
  "comment": "Hyperparameter configuration for training DQN agent in Cuttle game.",
  "random_seed": 15796,
  "comment_random_seed": "Seed for reproducibility. Set to null for non-deterministic runs. Experiment manager overrides this.",
  "network_type": "game_based",
  "comment_network_type": "Options: 'linear' (no hidden layers), 'large_hidden' (1024 neurons), 'game_based' (hierarchical [52,13,15]). Backward compatible: 'boolean', 'embedding', 'multi_encoder'",
  "use_embeddings": true,
  "comment_use_embeddings": "Use embeddings for preprocessing instead of boolean inputs. Previous experiments showed embeddings achieve 62% win rate vs 18% for boolean. This allows fair architecture comparison while using a representation that actually learns.",
  "embedding_dim": 52,
  "comment_embedding_dim": "Dimension of card embeddings. Game-based: 52 (one per card). Used for EmbeddingBasedNeuralNetwork when use_embeddings=true.",
  "zone_encoded_dim": 52,
  "comment_zone_encoded_dim": "Dimension of each zone encoding after aggregation. Game-based: 52 (one per card).",
  "game_based_scale": 2,
  "comment_game_based_scale": "Scale factor for game-based architecture. When set, uses [52*k, 13*k, 15*k] hidden layers. Increased from 1 to 2 to address catastrophic loss divergence. [104, 26, 30] provides 2x capacity and less severe bottleneck.",
  "game_based_hidden_layers": null,
  "comment_game_based_hidden_layers": "Optional explicit game-based hidden layer sizes (list of ints). If null, uses [52*scale, 13*scale, 15*scale]. Alternative: [104, 52, 26] for wider bottleneck (less compression).",
  "embedding_size": 16,
  "comment_embedding_size": "Kept for backward compatibility, no longer used (no embeddings - all inputs are boolean arrays)",
  "batch_size": 128,
  "gamma": 0.9,
  "eps_start": 0.9,
  "eps_end": 0.15,
  "eps_decay": 20000,
  "eps_decay_comment": "Decay constant for epsilon-greedy. Adjusted for ~34,000 total training steps. Epsilon decays from 0.9 to ~0.1 over full training, allowing gradual exploration-to-exploitation transition. Previous value (1500) caused epsilon to reach minimum too early (round 5).",
  "tau": 0.005,
  "target_update_frequency": 500,
  "learning_rate": 5e-5,
  "learning_rate_comment": "Reduced from 1e-4 to 3e-5 for more stable learning. Model was learning (loss decreasing) but not improving performance, suggesting unstable updates. Quick test mode enabled for debugging.",
  "lr_decay_rate": 0.9,
  "lr_decay_interval": 5,
  "gradient_clip_norm": 5.0,
  "q_value_clip": 15.0,
  "replay_buffer_size": 30000,
  "training": {
    "rounds": 20,
    "eps_per_round": 250,
    "quick_test_mode": false,
    "quick_test_mode_comment": "Disabled for full experiment. Set to true for quick debugging runs.",
    "quick_test_rounds": 5,
    "quick_test_eps_per_round": 250,
    "validation_episodes_ratio": 1.0,
    "validation_episodes_ratio_comment": "250 validation episodes per round for reliable win rate estimates. Time saved by reducing runs per type from 7 to 5 instead.",
    "validation_opponent": "both",
    "validation_opponent_comment": "Options: 'randomized', 'gapmaximizer', or 'both'"
  },
  "early_stopping": {
    "enabled": true,
    "check_interval": 50,
    "window_size": 100,
    "divergence_threshold": 0.5,
    "min_episodes": 200,
    "max_loss": 50.0
  }
}